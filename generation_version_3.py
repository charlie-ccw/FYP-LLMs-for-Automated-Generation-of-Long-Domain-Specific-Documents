"""
This is version 3 of the system. Building on version 2, it adds a RAG (Retrieval-Augmented Generation) step,
which introduces historical file samples to guide the model in further optimizing the content generated by version 2.
This enhancement primarily improves the overall structure and coherence.
"""
import asyncio
import json
import os.path

from langchain_core.documents import Document

from generation_version_2 import generation_version_2
from tools.retrieval_refine_with_target_text_tool import RetrievalRefineWithTargetTextTool
from util.chroma_db_util import ChromaDBUtil


async def generation_version_3(file_name_without_extension: str):
    print(f">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> {file_name_without_extension} <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< version 3")

    # Load the Template structure
    template_file_path = "project_template/template_version_1.json"
    with open(template_file_path, 'r', encoding='utf-8') as f:
        doc_template = json.load(f)

    # Load the draft generations for current file
    draft_file_path = os.path.join("generated_file/version_2", f"{file_name_without_extension}.json")
    # Generate the draft version of file
    if not os.path.isfile(draft_file_path):
        await generation_version_2(file_name_without_extension=file_name_without_extension)
    with open(draft_file_path, 'r', encoding='utf-8') as f:
        draft_file = json.load(f)

    # Load the target file extraction for current file to retrieve golden sample
    target_file_path = os.path.join('file/Energy_demand_extract/structure_1', f"{file_name_without_extension}.json")
    with open(target_file_path, 'r', encoding='utf-8') as f:
        target_file = json.load(f)

    # Create the refine tool object for function call
    retrieval_refine_with_target_text_tool = RetrievalRefineWithTargetTextTool()

    # Generation start
    file_generation = {}
    for chapter_id, chapter_template in doc_template.items():
        chapter_name = chapter_template['name']
        sections_template = chapter_template['sections']
        for section_id, section_template in sections_template.items():
            # Get draft section name and text for current section
            draft_section_name = draft_file[section_id]['section_name']
            draft_section_text = draft_file[section_id]['generation']

            # Jump to next section if original file does not contain current section
            if section_id not in target_file.keys():
                file_generation[section_id] = {
                    'section_name': draft_section_name,
                    'generation': draft_section_text
                }
                continue

            # Get the target section name and sample text for current section
            target_section_name = target_file[section_id]['section_name']
            target_section_text = target_file[section_id]['section_info']

            # Check if section name is correct
            if draft_section_name.lower() == target_section_name.lower():
                # Call refine tool to optimise the draft section text
                new_generation = await retrieval_refine_with_target_text_tool.acall(
                    target_text=target_section_text,
                    draft_text=draft_section_text,
                    knowledge_base=f'version3/{draft_section_name}')
                file_generation[section_id] = {
                    'section_name': draft_section_name,
                    'generation': new_generation['optimised_text']
                }
            else:
                file_generation[section_id] = {
                    'section_name': draft_section_name,
                    'generation': draft_section_text
                }

    # Write into correct JSON file
    with open(f"generated_file/version_3/{file_name_without_extension}.json", 'w', encoding='utf-8') as f:
        json.dump(file_generation, f, indent=4)


def build_the_train_knowledge_bases(train_files: list[str]):
    # Load the Template structure
    template_file_path = "project_template/template_version_1.json"
    with open(template_file_path, 'r', encoding='utf-8') as f:
        doc_template = json.load(f)

    # Initialise the sections knowleges
    sections_knowledge = {}
    for chapter_id, chapter_template in doc_template.items():
        sections_template = chapter_template['sections']
        for section_id, section_template in sections_template.items():
            section_name = section_template['name']
            sections_knowledge[section_name] = []

    # Start iteration and store section infos of each file into sections_knowledge for building knowledge bases
    for train_file in train_files:
        # Set the file path and Load the extracted infos
        file_path = os.path.join("file/Energy_demand_extract/structure_1", f"{train_file}.json")
        with open(file_path, 'r', encoding='utf-8') as f:
            train_datas = json.load(f)
        # Store each section info into specific item of sections_knowledge
        for section_id, section_info in train_datas.items():
            sections_knowledge[section_info['section_name']].append(Document(
                page_content=section_info['section_info'],
                metadata={'source': train_file}
            ))

    # Get the ChromaDB Tool
    chroma_db_tool = ChromaDBUtil()
    # Start iteration and build knowledge base for each section
    for section_name, documents in sections_knowledge.items():
        chroma_db_tool.initialise_vectorstore_with_documents(persist_directory=f"version3/{section_name}",
                                                             documents=documents)


async def main():
    # Load the train and test files with summary
    train_test_file_path = "project_template/template_version_1_summary.json"
    with open(train_test_file_path, 'r', encoding='utf-8') as f:
        train_test_datasets = json.load(f)

    # Get the train files
    train_datasets = train_test_datasets['train']
    # Check if the knowledge base of version3 exists and construct it.
    if not os.path.isdir("vectorDB/chromaDB/version3"):
        build_the_train_knowledge_bases(train_files=train_datasets)

    # Get the test files with summary
    test_datasets = train_test_datasets['test']

    # Asynchronous call for generating tasks for 9 files each time
    files_list = []
    for idx, test_dataset in enumerate(test_datasets):
        file_name_without_extension = test_dataset['file_name']
        files_list.append(file_name_without_extension)

        if len(files_list) == 9:
            tasks = [generation_version_3(file_name) for file_name in files_list]
            await asyncio.gather(*tasks)
            files_list = []

    # Process any remaining files
    if files_list:
        tasks = [generation_version_3(file_name) for file_name in files_list]
        await asyncio.gather(*tasks)


if __name__ == "__main__":
    asyncio.run(main())
